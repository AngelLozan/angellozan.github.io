---
lang: pt
page_id: local_llm
permalink: /posts/run-an-llm-locally
title: Executando DeepSeek r1 Localmente
date: 2025-03-13
categories: [Docker, AI, LLM, DeepSeek]
tags: [Tutorial, AI, Docker]
author: Angel
description: Execute DeepSeek Localmente com Docker-Compose online e offline
image: /assets/img/deepseek.png
---

# Execute DeepSeek Localmente com Docker-Compose
<p align="center">
    <img src="/assets/img/image-13.png" alt="alt text">
</p>

Executar DeepSeek localmente com Docker-Compose é possível com um Mac, embora uma implementação mais leve do modelo seja recomendada.

Isso irá orientá-lo sobre como executar DeepSeek no localhost com uma interface web-ui. 

Código encontrado aqui: [https://github.com/AngelLozan/Local_Deep_Seek](https://github.com/AngelLozan/Local_Deep_Seek)


<div class="d-flex justify-content-center align-items-center">
<p class="m-2">
    <img src="/assets/img/deepseek.png" alt="alt text">
</p>

<p class="m-2">
    <img src="/assets/img/image-1.png" alt="alt text">
</p>
</div>

<hr/>

## Execute com interface web

**Estes passos requerem conexão com internet**

1. Instale [Ollama](https://martech.org/how-to-run-deepseek-locally-on-your-computer/)

2.  Escolha um modelo baseado no seu hardware:
```
ollama pull deepseek-r1:8b  # Rápido, leve  

ollama pull deepseek-r1:14b # Desempenho equilibrado  

ollama pull deepseek-r1:32b # Processamento pesado  

ollama pull deepseek-r1:70b # Máximo raciocínio, mais lento

ollama pull deepseek-coder:1.3b # Assistente de conclusão de código
```
3. Teste o modelo localmente via terminal 
```
ollama run deepseek-r1:8b
```

4. Instale [Docker](https://www.docker.com/get-started)

5. Instale [Docker-Compose](https://formulae.brew.sh/formula/docker-compose)

6. Crie arquivo Docker-Compose como visto neste repo. Se você desejar usar uma conexão com internet, pode simplesmente descomentar a imagem para o serviço open-webui e remover o build. 

7. Abra o app docker e execute `docker-compose up --build`

8. Visite `http://localhost:3000` para ver seu chat. 

<hr/>

## Execute com VScode (offline):

1. Siga os passos 1-2 em [Passos para executar com interface web](#execute-com-interface-web), então você também pode instalar a extensão CodeGPT para VScode. 
![alt text](/assets/img/image-2.png)
![alt text](/assets/img/image-3.png)

2. Navegue até a seção Local LLMs. Isso provavelmente é acessado a partir do menu suspenso de seleção de modelo inicial (mostrado com claude selecionado).
![alt text](/assets/img/image-9.png)
![alt text](/assets/img/image-10.png)

3. Das opções disponíveis, selecione 'Ollama' como provedor de LLM local.
![alt text](/assets/img/image-11.png)

4. Selecione seu Modelo DeepSeek e pronto.
![alt text](/assets/img/image-12.png)

5. Agora você pode desligar a internet e usando LLMs Locais, continuar a conversar/analisar código. 

<hr/>

## Executando Open-webui localmente sem internet

1. Siga os passos 1-2 em [Passos para executar com interface web](#execute-com-interface-web)

2. Instale `uv` `curl -LsSf https://astral.sh/uv/install.sh | sh`

3. Crie ambiente uv:  `mkdir ~/< project root >/< your directory name> && uv venv --python 3.11`

4. Instale open-webui: `cd ~/< project root >/< your directory name > && uv pip install open-webui`

5. Inicie open-webui: `DATA_DIR=~/.open-webui uv run open-webui serve`

6. Visite localhost e comece a conversar!

<hr/>

## Executando localmente via docker sem internet com Open-webui

1. Siga os passos 1-6 em [passos para executar com interface web](#execute-com-interface-web)

2. Em seguida, siga os passos 1-4 em [passos para executar open-webui localmente sem internet](#executando-open-webui-localmente-sem-internet)

3. Uma vez feito isso, crie um `Dockerfile` no seu diretório escolhido onde as dependências do open-webui residem como visto neste projeto, para imitar a configuração e instalação no container docker de todas as dependências para open-webui.

4. Em seguida, inicie o app: `docker-compose up --build`. Se você não deseja ver logs: `docker-compose up --build -d`

5. Visite localhost e comece a conversar!

6. Se modelos não estiverem disponíveis para seleção, ligue sua internet temporariamente, volte ao terminal e execute `docker exec -it ollama bash`

7. Baixe o modelo para seu serviço usando os comandos `ollama pull` vistos anteriormente no passo 1. 

8. Verifique se os modelos estão instalados com `ollama list` enquanto ainda estiver no cli. Se sim, você pode desligar a internet novamente e sair do cli com `ctrl + d` ou `exit`

9. Reinicie seu container `open-webui` com `docker-compose restart open-webui`

<hr/>

## Solução de Problemas

1. Inspecione a rede: `docker network ls` depois `docker network inspect < network >`

2. Inspecione Ollama e modelos: 

`curl http://localhost:11434/api/tags`

- ou -

`docker exec -it ollama ollama list`

3. Reinicie container open-webui: `docker-compose restart open-webui`

4. Dependendo do seu hardware, executar `docker-compose down` depois `docker-compose up -d` para reiniciar containers construídos pode levar um momento. Verifique o progresso com `docker logs < service name >`
